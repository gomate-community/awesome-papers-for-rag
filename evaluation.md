
## 1. Survey papers <a id="survey"></a>

| **Date** | **Title** | **Organization**  |  **Code**  |
| :-----------: | :-------------: | :----------------------: |  :----------------------: |
|2025/04/10|[LLM-based NLG Evaluation: Current Status and Challenges](https://direct.mit.edu/coli/article/doi/10.1162/coli_a_00561/128807)|Peking University|-|
|2024/05/13|[Evaluation of Retrieval-Augmented Generation: A Survey](https://arxiv.org/pdf/2405.07437)|Tencent|[Code](https://github.com/vasiliskatr/faaf)<br>![](https://img.shields.io/github/stars/YHPeter/Awesome-RAG-Evaluation.svg?style=social)|
|2024/01/30|[RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture](https://arxiv.org/abs/2401.08406)|Microsoft|No|


## 2 Evaluation papers <a id="short"></a>
<br>

### 2.1 Short answer evaluation <a id="aspect"></a>

### 2.2 Long answer evaluation <a id="short"></a>

| **Date** | **Title** | **Organization**  |  **Method**  | **Metric**  | **Dataset** |  **Code**  |
| :-----------: | :-------------: | :----------------------: |  :----------------------: |:----------------------: |:----------------------: |:----------------------: |
|2023/09/15|[Investigating Answerability of LLMs for Long-Form Question Answering](https://arxiv.org/pdf/2309.08210)|Salesforce|**Prompting GPT-4** to rate answers on a scale from 0 to 3.| <details><summary><small>Coherency, Relevance, Factual consistency, and Accuracy</small></summary><small> **Coherency**: Answer should be well-structured and well-organized and should not just be a heap of related information. **Relevance**: Answer should be relevant to the question and the context. The answer should be concise and avoid drifting from the question being asked. **Factual consistency**: The context should be the primary source for the answer. The answer should not contain fabricated facts and should entail information present in the context. **Accuracy**: Answer should be satisfactory and complete to the question being asked. Measure the correctness of the answer by checking if the response answers the presented question.</small></details> |-|-|
 
### 2.3 Context evaluation <a id="short"></a>

### 2.4 Documents evaluation <a id="short"></a>

## 3 Tools and Benchmarks <a id="datasets"></a>

| **Date** | **Title** | **Organization**  |  **Code**  |
| :-----------: | :-------------: | :----------------------: |  :----------------------: |
|2024/10/10|[HELMET: How to Evaluate Long-Context Language Models Effectively and Thoroughly](https://arxiv.org/pdf/2410.02694?)|Princeton|[Code](https://github.com/princeton-nlp/HELMET)<br>![](https://img.shields.io/github/stars/princeton-nlp/HELMET.svg?style=social)|
|2024/08/16|[RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models](https://aclanthology.org/2024.acl-long.585)                               |                    NewsBreak                    |           [Code](https://github.com/ParticleMedia/RAGTruth) <br>![](https://img.shields.io/github/stars/ParticleMedia/RAGTruth.svg?style=social)           |
|2024/08/16|[RAGChecker: A Fine-grained Framework for Diagnosing Retrieval-Augmented Generation](http://arxiv.org/abs/2408.08067)                               |                    Amazon                    |           [Code](https://github.com/amazon-science/RAGChecker) <br>![](https://img.shields.io/github/stars/amazon-science/RAGChecker.svg?style=social)           |
|2024/04/21|[Evaluating Retrieval Quality in Retrieval-Augmented Generation](https://arxiv.org/pdf/2404.13781)|UMASS|No|
|2024/04/08|[FaaF: Facts as a Function for the evaluation of generated text](https://arxiv.org/pdf/2403.03888)|IMMO Capital|[Code](https://github.com/vasiliskatr/faaf)<br>![](https://img.shields.io/github/stars/vasiliskatr/faaf.svg?style=social)|
|2024/02/19|[CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/abs/2401.17043)|University of Science and Technology of China|[Code](https://github.com/IAAR-Shanghai/CRUD_RAG)<br>![](https://img.shields.io/github/stars/IAAR-Shanghai/CRUD_RAG.svg?style=social)|
|2024/1/11|[Seven Failure Points When Engineering a Retrieval Augmented Generation System](https://arxiv.org/abs/2401.05856)|Applied Artificial Intelligence Institute|No
|2023/12/20|[Benchmarking Large Language Models in Retrieval-Augmented Generation](https://arxiv.org/abs/2309.01431)|Chinese Information Processing Laboratory|[Code](https://github.com/chen700564/RGB)<br>![](https://img.shields.io/github/stars/chen700564/RGB.svg?style=social)|
|2023/11/16|[ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2311.09476)|Stanford|[Code](https://github.com/stanford-futuredata/ARES)<br>![](https://img.shields.io/github/stars/stanford-futuredata/ARES.svg?style=social)|
|2023/11/14|[RECALL: A Benchmark for LLMs Robustness against External Counterfactual Knowledge](https://arxiv.org/abs/2311.08147)|Peking University|No|
|2023/10/31|[Enabling Large Language Models to Generate Text with Citations](https://arxiv.org/abs/2305.14627)|Princeton University|[Code](https://github.com/princeton-nlp/ALCE)<br>![](https://img.shields.io/github/stars/princeton-nlp/ALCE.svg?style=social)|
|2023/09/26|[RAGAS: Automated Evaluation of Retrieval Augmented Generation](https://arxiv.org/abs/2309.15217)|Exploding Gradients|[Code](https://github.com/explodinggradients/ragas)<br>![](https://img.shields.io/github/stars/explodinggradients/ragas.svg?style=social)|
|2021/08/05|[TruLens:Evaluation and Tracking for LLM Experiments](https://www.trulens.org/)|TruEra|[Code](https://github.com/truera/trulens)<br>![](https://img.shields.io/github/stars/truera/trulens.svg?style=social)|


