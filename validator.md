
## Introduction

The answer generated by LLMs are tend to be haullucinations. To overcome this, it would be better to introduce an additional component to increase the quality of the answer, namely **answer enhancement** component. The



## 1. Answer Verifiaction <a id="verify"></a>

### 1.1 Attribution Detection <a id="attribution"></a>

| Date       | Title                                                                                                           | Authors                                  | Orgnization                                                                                                   | Abs                                                                                             |
|------------|-----------------------------------------------------------------------------------------------------------------|------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|
|2023/10/07| [Automatic Evaluation of Attribution by Large Language Models](https://arxiv.org/pdf/2305.06311.pdf) <br> [[code](https://github.com/OSU-NLP-Group/AttrScore): ![](https://img.shields.io/github/stars/OSU-NLP-Group/AttrScore.svg?style=social) |Xiang Yue, Boshi Wang, Ziru Chen, et. al.|The Ohio State University | <details><summary><small>This paper presents evaluation...</small></summary><small>This work tries to evaluate the attribution ability (3 types: attributable, extrapolatory, contradictory) of existing LLMs by introducing two benchmarks (i.e., AttrEval-Simulation and AttrEval-GenSearch). It also introduces two types of automatic evaluation methods: 1) Prompting LLMs, 2) Fine-tuning LMs on Repurposed Data. </small></details>|

### 1.2 Claim Verification <a id="verification"></a>

| Date       | Title                                                                                                           | Authors                                  | Orgnization                                                                                                   | Abs                                                                                             |
|------------|-----------------------------------------------------------------------------------------------------------------|------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|
|2024/12/16| [Attention with Dependency Parsing Augmentation for Fine-Grained Attribution](https://arxiv.org/pdf/2412.11404) <br> |Qiang Ding, Lvzhou Luo, Yixuan Cao, Ping Luo |ICT| <details><summary><small>This paper presents fine-grained attribution ...</small></summary><small>This work proposes two techniques to **model-internals-based** methods for fine-grained attribution. First, it aggregates token-wise evidence (i.e., attention weights) through set union operations, preserving the granularity of representations. Second, it enhances the attributor by integrating dependency parsing to enrich the semantic completeness of target spans. </small></details>|
|2024/07/02| [Pelican: Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification](https://arxiv.org/pdf/2407.02352.pdf) <br> |Pritish Sahu, Karan Sikka, Ajay Divakaran|SRI International, Princeton| <details><summary><small>This paper presents **Pelican** ...</small></summary><small>Pelican 1) decomposes the visual claim into a chain of sub-claims based on first-order predicates, 2) it then use Program-of-Thought prompting to generate Python code for answering these questions through flexible composition of external tools. </small></details>|
|2024/02/23 |[Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature of Aggregated Factual Claims in Long-Form Generations](https://arxiv.org/abs/2402.05629.pdf)| heng-Han Chiang, Hung-yi Lee.|National Taiwan University|<details><summary><small>This paper presents **D-FActScore** ...</small></summary><small>This work finds that combining factual claims together can result in a non-factual paragraph due to entity ambiguity. Current metrics for fact verification fail to properly evaluate these non-factual passages. The authors proposed D-FActScore based on FActScore, and showed the methods and results of human and automatic evaluation.</small></details>|
|2023/10/20| [Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models](https://arxiv.org/abs/2310.05253.pdf) <br> [[code](https://github.com/wang2226/FOLK): ![](https://img.shields.io/github/stars/wang2226/FOLK.svg?style=social) |Haoran Wang, Kai Shu|Illinois Institute of Technology, Chicago| <details><summary><small>This paper presents FOLK ...</small></summary><small>This work introduces First-Order-Logic-Guided Knowledge-Grounded (**FOLK**). 1）FOLK translates input claim into a FOL clause and uses it to guide LLMs to generate a set of question-answer pairs, 2) FOLK then retrieves knowledge-grounded answers from external knowledge-source; 3) FOLK performs FOL-guided reasoning over knowledge-grounded answers to make veracity prediction and generate explanations.</small></details>|

## 2. Reasoning-based (CoT) Filtering <a id="cot"></a>

| Date       | Title                                                                                                           | Authors                                  | Orgnization                                                                                                   | Abs                                                                                             |
|------------|-----------------------------------------------------------------------------------------------------------------|------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|
|2023/12/31| [Rethinking with Retrieval: Faithful Large Language Model Inference](https://arxiv.org/abs/2301.00303.pdf) <br> [[code](https://github.com/HornHehhf/RR): ![](https://img.shields.io/github/stars/HornHehhf/RR.svg?style=social) |Hangfeng He, Hongming Zhang, Dan Roth|University of Rochester, Tencent AI Lab Seattle, University of Pennsylvania | <details><summary><small>This paper presents RR ...</small></summary><small>This work propose a novel post-processing approach, rethinking with retrieval (RR), which uses decomposed reasoning steps obtained from CoT prompting to retrieve relevant docs for LLMs. Four steps: 1)CoT prompting to generate explanation E and prediction P for query Q. 2)Sampling diverse reasoning path R (i.e., E + P), 3)knowledge K retrieval for each path, 4)faithful inference (NLI model) for each R+K.</small></details>|

## 3. Datasets <a id="datasets"></a>


| Date       | Title | Authors   | Orgnization | Abs    | Dataset                                                                                           |
|------------|-----------------------------------------------------------------------------------------------------------------|------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|
|2025/04/11 | [Attribution in Scientific Literature: New Benchmark and Methods](https://arxiv.org/pdf/2405.02228)<br> [[code](https://github.com/YashSaxena21/REASONS): ![](https://img.shields.io/github/stars/YashSaxena21/REASONS.svg?style=social)| Yash Saxena, Deepa Tilwani, Seyedali Mohammadi, et. al.|UMBC|<details><summary><small>This paper presents **REASONS** ...</small></summary><small> REASONS is a novel dataset for source attribution, featuring sentence-level annotations across 12 scientific domains from arXiv.</small></details>| <sub> Factcheck-bench </sub>|
|2024/07/11 | [CiteME: Can Language Models Accurately Cite Scientific Claims?](https://proceedings.neurips.cc/paper_files/paper/2024/file/0ef47f7b768e1a012e3d995ac8d8fac7-Paper-Datasets_and_Benchmarks_Track.pdf)<br> [[code](https://github.com/bethgelab/CiteME): ![](https://img.shields.io/github/stars/bethgelab/CiteME.svg?style=social)| Ori Press, Andreas Hochlehnert, Ameya Prabhu, et al.| University of Tübingen|<details><summary><small>This paper presents **CiteME** ...</small></summary><small> CiteME, a challenging and human-curated benchmark of recent machine learning publications that evaluates the abilities of LMs to correctly attribute scientific claims. CiteME is both natural and challenging, even for SoTA LMs.</small></details>| <sub> CiteME </sub>|
|2024/04/16 | [Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-checkers](https://arxiv.org/pdf/2311.09000)| Yuxia Wang, Revanth G. Reddy, Zain M. Mujahid, et. al.|MBZUAI, Abu Dhabi, UAE|<details><summary><small>This paper presents Factcheck-Bench ...</small></summary><small>Factcheck-bench is a open-domain document-level factuality benchmark in three-level granularity: claim, sentence and document. They frame the automated detection and correction of factual errors for outputs of LLMs into eight subtasks: 1)decomposition; 2) decontextualisation; 3) checkworthiness  identification; 4) evidence retrieval and collection; 5) stance detection; 6) correction determination; 7) claim correction; 8) final response revision.</small></details>| <sub> Factcheck-bench </sub>|
