
## Introduction

The retrieved documents often contain a list of passages which are ranked by their relevance score to the question. It would be costly to directly input these passages into the LLM. On one hand, <u>the relevance score of these passages does not necessarily indicate their usefulness for answer generation, which could introduce noise to the LLM</u>. On the other hand, <u>the length of the list could exceed the length limit of the LLM</u>. Thus, the mediation (also called post-retrieval) component is introduced to select or compress the retrieved content.



## 1. Survey papers <a id="survey"></a>

| Date       | Title | Authors   | Orgnization | Abs    |                                                                           
|------------|-----------------------------------------------------------------------------------------------------------------|------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|
|2024/10/02 <br> (ðŸŒŸ)| [Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/pdf/2409.13385) |Sourav Verma|IBM Watsonx Client Engineering, India |<details><summary><small> Survey on contextual compression.</small></summary><small>**Contextual compression for large language models**: semantic compression, pre-trained language models, retrievers.</small></details>|


## 2. Compression-based Adapter <a id="compress"></a>
### 2.1 Selective Methods <a id="selection"></a>
Selective Methods aims to select a subset of tokens from original contexts, to filter out noises in the context.

| Date       | Title | Authors   | Orgnization | Abs    | Dataset                                                                                           |
|------------|-----------------------------------------------------------------------------------------------------------------|------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|
|2024/12/18<br> (ðŸŒŸðŸŒŸðŸŒŸðŸŒŸ)| [EXIT: Context-Aware Extractive Compression for Enhancing Retrieval-Augmented Generation](https://arxiv.org/abs/2412.12559) <br>[[code](https://github.com/ThisIsHwang/EXIT): ![](https://img.shields.io/github/stars/ThisIsHwang/EXIT.svg?style=social)]|Taeho Hwang, Sukmin Cho, Soyeong Jeong, et al.|Korea Advanced Institute of Science and Technology|<details><summary><small>**EXIT**.</small></summary><small>**EXIT (EXtractIve ContexT compression)** operates in three stages: 1) splitting retrieved documents into sentences (rule-based), 2ï¼‰performing parallelizable binary classification ("YES" or "NO") on each sentence (Gemma-2B-it), 3) recombining selected sentences while preserving their orginial order. (LLaMA3.1-8B)</small></details>| <sub>NQ, TriviaQA, HotpotQA, 2WikiMultiHopQA</sub> |
|2024/03/21 <br> (ðŸŒŸðŸŒŸ)| [FIT-RAG: Black-Box RAG with Factual Information and Token Reduction](https://arxiv.org/pdf/2403.14374) |Yuren Mao, Xuemei Dong, Wenyi Xu, et al. |Zhejiang University |<details><summary><small>This paper presents FIT-RAG ...</small></summary><small>**FIT-RAG** utilizes the factual information in the retrieval and reduces the number of tokens for augmentation. It consists of five components: a similarity-based retriever, a bi-label document scorer, a bi-faceted self-knowledge recognizer, a sub-document-level token reducer and a prompt construction module.</small></details>| <sub>TriviaQA, NQ, PopQA</sub> |
|2024/03/19 <br> (ðŸŒŸðŸŒŸðŸŒŸ)| [LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression](https://arxiv.org/abs/2403.12968) <br>[[code](https://github.com/microsoft/LLMLingua): ![](https://img.shields.io/github/stars/microsoft/LLMLingua.svg?style=social)]|Zhuoshi Pan, Qianhui Wu, et al. |Microsoft Corporation |<details><summary><small>**LLMLingua-2**.</small></summary><small>**LLMLingua-2** formulate prompt compression as a token classification problem to guarantee the faithfulness of the compressed prompt to the original one, and use a Transformer encoder as the base architecture to capture all essential information for prompt compression from the full bidirectional context.  </small></details>| <sub>MeetingBank, LongBench, ZeroScrolls, GSM8K, BBH</sub> |
|2023/11/14 <br> (ðŸŒŸðŸŒŸðŸŒŸðŸŒŸ)| [Learning to Filter Context for Retrieval-Augmented Generation](https://arxiv.org/pdf/2311.08377) <br>[[code](https://github.com/zorazrw/filco): ![](https://img.shields.io/github/stars/zorazrw/filco.svg?style=social)]|Zhiruo Wang, Jun Araki, et al. |Carnegie Mellon University |<details><summary><small>**FILCO**.</small></summary><small>**FILCO** improves the quality of the context provided to the generator by (1) identifying useful context based on lexical and information-theoretic approaches, and (2) training context filtering models that can filter retrieved contexts at test time.</small></details>| <sub>NQ, TriviaQA, ELI5, FEVER, WoW</sub> |
|2023/10/10 <br> (ðŸŒŸðŸŒŸðŸŒŸ)| [LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression](https://arxiv.org/abs/2310.06839) <br>[[code](https://github.com/microsoft/LLMLingua): ![](https://img.shields.io/github/stars/microsoft/LLMLingua.svg?style=social)]|Huiqiang Jiang, Qianhui Wu, et al. |Microsoft Corporation |<details><summary><small>**LongLLMLingua**.</small></summary><small>**LongLLMLingua** conducts prompt compression towards improving LLMsâ€™ perception of the key information to address three challenges: higher computational/financial cost, longer latency, and inferior performance. </small></details>| <sub> LongBench, ZeroSCROLLS, MuSiQue, LooGLE</sub> |
|2023/10/09 <br> (ðŸŒŸðŸŒŸðŸŒŸðŸŒŸ)| [LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models](https://arxiv.org/abs/2310.05736)<br>[[code](https://github.com/microsoft/LLMLingua): ![](https://img.shields.io/github/stars/microsoft/LLMLingua.svg?style=social)] |Huiqiang Jiang, Qianhui Wu, et al. |Microsoft Corporation |<details><summary><small>**LLMLingua**: LLaMA-7B to identify and remove non-essential tokens.</small></summary><small>**LLMLingua** is a coarse-to-fine prompt compression method that involves a budget controller to maintain semantic integrity under high compression ratios, a token-level iterative compression algorithm to better model the interdependence between compressed contents, and an instruction tuning based method for distribution alignment between language models.  </small></details>|  <sub>GSM8K, BBH, ShareGPT, and Arxiv-March23</sub> |
|2023/09/02 <br> (ðŸŒŸðŸŒŸ)| [LeanContext: Cost-Efficient Domain-Specific Question Answering Using LLMs](https://arxiv.org/abs/2309.00841) |Md Adnan Arefeen, Biplob Debnath, Srimat Chakradhar |NEC Laboratories America |<details><summary><small>**LeanContext**: ranking sentence with cosine.</small></summary><small>**LeanContext** extracts *k key sentences* from the context that are closely aligned with the query. LeanContext introduces a reinforcement learning technique that dynamically determines k based on the query and context. The rest of the less important sentences are reduced using a free open source text reduction method. </small></details>| <sub>Arxiv, BBC News</sub> |
|2023/04/24 <br> (ðŸŒŸðŸŒŸðŸŒŸðŸŒŸ)| [Compressing Context to Enhance Inference Efficiency of Large Language Models](https://aclanthology.org/2023.emnlp-main.391.pdf) <br>[[code](https://github.com/lliyucheng09/Selective_Context): ![](https://img.shields.io/github/stars/liyucheng09/Selective_Context.svg?style=social)]|Yucheng Li, Bo Dong, Frank Guerin, Chenghua Lin |University of Surrey, University of Manchester, University of Sheffield, UK|<details><summary><small>**Selective_Context**: LLaMA-7B token probabilities</small></summary><small>**Selective_Context** evaluates informativeness of *lexical units (i.e., tokens, phrases, or sentences)* with self-information computed by a base causal language model. It selectively retains content with higher self-information.</small></details>| <sub>axXiv papers, BBC News, ShareGPT.com</sub> |



### 2.2 Abstractive Methods <a id="abstractive"></a>

Abstractive Methods usually compress contexts by generating summarys, to filter out noises in the context.

| Date       | Title | Authors   | Orgnization | Abs    | Dataset                                                                                           |
|------------|-----------------------------------------------------------------------------------------------------------------|------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|
|2025/05/21<br> (ðŸŒŸðŸŒŸðŸŒŸ)| [Beyond Hard and Soft: Hybrid Context Compression for Balancing Local and Global Information Retention](https://arxiv.org/abs/2505.15774) <br>[[code](https://github.com/Xnhyacinth/HyCo2): ![](https://img.shields.io/github/stars/Xnhyacinth/HyCo2.svg?style=social)]| Huanxuan Liao, Wen Hu, Yao Xu, et al.| Institute of Automation, CAS| <details><summary><small>**HyCo2**:finetune, LLaMA3.1-8B.</small></summary><small>**HyCo2** integrates global and local perspectives to guide context compression. It uses a hybrid adapter to refine global semantics, and incorporates a classification layer to assign a retension probability to each token on the local view.</small></details>| <sub>NQ, TriviaQA, WebQuestions, PopQA, ComplexWebQuestions, HotpotQA, 2WikiMultihopQA </sub> |
|2024/10/14<br> (ðŸŒŸðŸŒŸðŸŒŸ)| [COMPACT: Compressing Retrieved Documents Actively for Question Answering](https://arxiv.org/abs/2407.09014) <br>[[code](https://github.com/dmis-lab/CompAct): ![](https://img.shields.io/github/stars/dmis-lab/CompAct.svg?style=social)]|Chanwoong Yoon,Taewhoo Lee, Hyeon Hwang, et al.|Korea University|<details><summary><small>**CompAct**: instruction-tuned Mistral-7B.</small></summary><small>**CompAct** groups documents into several segments, then sequentially compress segments into a compacted context. It uses a subset of HotpotQA training set for data collection, and utilizes GPT-4o API to collect dataset.</small></details>| <sub>NQ, TriviaQA, HotpotQA, 2WikiMultiHopQA, MuSiQue</sub> |
|2024/07/04<br> (ðŸŒŸðŸŒŸðŸŒŸ)| [Attribute First, then Generate: Locally-attributable Grounded Text Generation](https://arxiv.org/abs/2403.17104) <br>[[code](https://github.com/lovodkin93/attribute-first-then-generate): ![](https://img.shields.io/github/stars/lovodkin93/attribute-first-then-generate.svg?style=social)]|Aviv Slobodkin, Eran Hirsch et al. |Bar-Ilan University|<details><summary><small>**AttrFirst**.</small></summary><small>**AttrFirst** propose a locally-attributable text generation approach, with prompt-based three steps: 1) content selection (choosing relevant spans from source texts), 2)sentence-level planning (organizing and grouping content), 3)sentence-by-sentence generation (based on selected and structured output).</small></details>| <sub>DUC, TAC, MultiNews </sub>|
|2024/02/15<br> (ðŸŒŸðŸŒŸðŸŒŸ)| [Grounding Language Model with Chunking-Free In-Context Retrieval](https://arxiv.org/abs/2402.09760) | Hongjin Qian, et al. | Gaoling School of Artificial Intelligence, Renmin University of China, | <details><summary><small>**CFIC**.</small></summary><small>This paper presents a novel Chunking-Free In-Context (CFIC) retrieval approach, specifically tailored for Retrieval-Augmented Generation (RAG) systems. CFIC addresses these challenges by circumventing the conventional chunking process. It utilizes the encoded hidden states of documents for in-context retrieval, employing auto-aggressive decoding to accurately identify the specific evidence text required for user queries, eliminating the need for chunking. CFIC is further enhanced by incorporating two decoding strategies, namely Constrained Sentence Prefix Decoding and Skip Decoding. These strategies not only improve the efficiency of the retrieval process but also ensure that the fidelity of the generated grounding text evidence is maintained.</small></details>| <sub>NarrativeQA, Qasper, MultiFieldQA, HotpotQA, MuSiQue</sub> |
|2023/10/25<br> (ðŸŒŸðŸŒŸ)| [TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost Reduction](https://arxiv.org/abs/2310.15556) |Junyi Liu, Liangzhi Li, et al. |Meetyou AI Lab|<details><summary><small>This paper presents TCRA...</small></summary><small>**TCRA** propose a token compression scheme that includes two methods: summarization compression and semantic compression. The first method applies a T5-based model that is fine-tuned by datasets generated using self-instruct containing samples with varying lengths and reduce token size by doing summarization. The second method further compresses the token size by removing words with lower impact on the semantic.</small></details>| <sub>FRDB</sub> |
|2023/04/12 <br> (ðŸŒŸðŸŒŸðŸŒŸ)| [RECOMP: Improving Retrieval-Augmented LMs With Compression and Selective Augmentation](https://arxiv.org/pdf/2310.04408.pdf) <br>[[code](https://github.com/carriex/recomp): ![](https://img.shields.io/github/stars/carriex/recomp.svg?style=social)]|Fangyuan Xu, Weijia Shi, Eunsol Choi1 |The University of Texas at Austin, University of Washington|<details><summary><small>**Recomp**.</small></summary><small>**Recomp** introduces two types of compressors: an <u>extractive</u> compressor that selects pertinent sentences from retrieved documents, and an <u>abstractive</u> compressor that produces concise summaries by amalgamating information from multiple documents.</small></details>| <sub>WikiText-103, NQ, TriviaQA, HotpotQA</sub> |

## 3. Thoughts-based Methods <a id="thoughts"></a>

| Date       | Title | Authors   | Orgnization | Abs    | Dataset                                                                                           |
|------------|-----------------------------------------------------------------------------------------------------------------|------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|
|2024/03/28 <br> (ðŸŒŸðŸŒŸðŸŒŸ)| [ActiveRAG: Autonomously Knowledge Assimilation and Accommodation through Retrieval-Augmented Agents](https://arxiv.org/pdf/2402.13547) [[code](https://github.com/OpenMatch/ActiveRAG): ![](https://img.shields.io/github/stars/OpenMatch/ActiveRAG.svg?style=social)]| Zhipeng Xu, Zhenghao Liu, Yukun Yan, et al.|Northeastern University, China|<details><summary><small>**ActiveRAG** ...</small></summary><small>The ActiveRAG workflow follows three-step: 1) the *Self-Inquiry Agent* produce chain-of-thought (P) to answer the question using LLM based on Q. 2) The *Knowledge Assimilation agent* generates an assimilation rational (T) based on Q and D. 3) The *Thought Accommodation agent* generates responses based on (Q, T,P).</small></details>| <sub>PopQA, TriviaQA, NQ, 2WikiMHQA, ASQA</sub> |
|2023/10/17 <br> (ðŸŒŸðŸŒŸðŸŒŸðŸŒŸ)| [SELF-RAG: Learning To Retrieve, Generate, and Critique  Through SELF-Reflection](https://arxiv.org/pdf/2310.11511) [[code](https://github.com/AkariAsai/self-rag): ![](https://img.shields.io/github/stars/AkariAsai/self-rag.svg?style=social)]|YAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi|University of Washington|<details><summary><small>**Self-RAG**.</small></summary><small>This work introduces a framework called Self-Reflective Retrieval-Augmented Generation that enhances an LMâ€™s quality and factuality through retrieval and self-reflection. It trains a single LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens.</small></details>| <sub>PubHealth, ARC-Challenge, PopQA, TRiviaQA, ALCE-ASQA</sub> |
|2023/10/8<br> (ðŸŒŸðŸŒŸ)| [Self-Knowledge Guided Retrieval Augmentation for Large Language Models](https://arxiv.org/abs/2310.05002) [[code](https://github.com/THUNLP-MT/SKR): ![](https://img.shields.io/github/stars/THUNLP-MT/SKR.svg?style=social)]|Yile Wang, Peng Li, Maosong Sun, Yang Liu| Tsinghua University|<details><summary><small>**SKR**.</small></summary><small>This work investigate eliciting the model's ability to recognize what they know and do not know (which is also called self-knowledge) and propose Self-Knowledge guided Retrieval augmentation (SKR), a simple yet effective method which can let LLMs refer to the questions they have previously encountered and adaptively call for external resources when dealing with new questions.</small></details>| <sub>TemporalQA, CommonsenseQA, TabularQA, StrategyQA, TruthfulQA</sub> |


## 4. Preference (Dual) Alignment Methods <a id="align"></a>

## 4.1. Finetuning-based Alignment <a id="finetune"></a>
Finetuning both retriever and generator to align them for better retrieval and generation, respectively.

| Date       | Title | Authors   | Orgnization | Abs    | Dataset                                                                                           |
|------------|-----------------------------------------------------------------------------------------------------------------|------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|
|2024/07/18 | [Understand What LLM Needs: Dual Preference Alignment for Retrieval-Augmented Generation](https://arxiv.org/pdf/2406.18676) [[code](https://github.com/dongguanting/DPA-RAG): ![](https://img.shields.io/github/stars/dongguanting/DPA-RAG.svg?style=social)]| Guanting Dong, Yutao Zhu, Chenghao Zhang, Zechen Wang, Zhicheng Dou, Ji-Rong Wen|Renmin University of China|<details><summary><small>This paper presents *DPA-RAG* ...</small></summary><small>DPA-RAG consists of three key components: (1) Preference Knowledge Construction: it first extracts the specific knowledge that significantly affects LLMsâ€™ reasoning preferences. Then we introduce five query augmentation strategies and a quality filtering process to synthesize high-quality preference knowledge. (2) Reranker-LLM Alignment: it designs multi-grained alignment tasks for fine-tuning a preference-aligned reranker. (3) LLM Self-Alignment: it introduces a pre-aligned phrase prior to the vanilla SFT stage.</small></details>| <sub>NQ, TriviaQA, HotpotQA, WebQSP</sub> |
|2023/5/24| [REPLUG: Retrieval-Augmented Black-Box Language Models](https://arxiv.org/abs/2301.12652.pdf) [[code](https://github.com/swj0419/REPLUG): ![](https://img.shields.io/github/stars/swj0419/REPLUG.svg?style=social)]|Weijia Shi, Sewon Min, Michihiro Yasunaga, et. al.|University of Washington, Stanford University, KAIST, Meta AI|<details><summary><small>This paper presents REPLUT ...</small></summary><small>This work introduce REPLUG, which prepends each retrieved document and question separately to the LLM and ensembles output probabilities from different passes. Besides, it takes LM to score documents to supervise the dense retriever training.</small></details>| <sub>Pile, NQ, TriviaQA</sub> |
|2022/11/16| [Atlas: Few-shot Learning with Retrieval Augmented Language Models](https://arxiv.org/abs/2208.03299.pdf)<br> [[code](https://github.com/facebookresearch/atlas): ![](https://img.shields.io/github/stars/facebookresearch/atlas.svg?style=social)]|Gautier Izacard, Patrick Lewis, Maria Lomeli, et. al.|Meta AI, ENS, PSL University, Inria, UCL|<details><summary><small>This paper presents Atlas ...</small></summary><small>This work present Atlas, a retrieval (Contriever) augmented language model (T5) by carefully designed training, i.e., 1) jointly pre-train the retriever and LLM using unsupervised output, 2) efficient retriever fine-tuning (including full index update, reranking, and query-side fine-tuning).</small></details>| <sub>KILT, MMLU, NQ, TriviaQA</sub> |


## 4.2. Interative-based Alignment <a id="iterative"></a>
Iterative between retriever and generator to align them for better retrieval and generation, respectively.


| Date       | Title | Authors   | Orgnization | Abs    | Dataset                                                                                           |
|------------|-----------------------------------------------------------------------------------------------------------------|------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|
|2025/01/24 | [Chain-of-Retrieval Augmented Generation](https://arxiv.org/pdf/2501.14342)  | Liang Wang, Haonan Chen, Nan Yang, et. al.| Microsoft Corporation, Renming University of China| <details><summary><small>This paper presents CoRAG ...</small></summary><small>This work propose **CoRAG**, simulates the iterative, step-by-step reasoning process to solve complex questions. It allows the model to refine its query, gather new insights, and synthesize information in a more structured way. They propose *rejection sampling* to generates intermediate retrieval chains (chain of sub-queries and sub-answers) to fine-tune an LLM using standard next-token prediction. </small></details>| <sub>HotpotQA,2WikiMHQA, Bamboogle, Musique, and the KILT benchmark.</sub> |
|2024/11/29 | [Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language Models](https://arxiv.org/abs/2411.19443) <br>[[code](https://github.com/ictnlp/Auto-RAG): ![](https://img.shields.io/github/stars/ictnlp/Auto-RAG.svg?style=social)] | Yu Tian, Shaolei Zhang, Yang Feng| ICT, CAS| <details><summary><small>This paper presents Auto-RAG ...</small></summary><small>This work propose **Auto-RAG**, which autonomously synthesizing reasoning-based decision-making instructions in iterative retrieval and fine-tuned the open-source LLMs. </small></details>| <sub>NQ, HotpotQA,2WikiMHQA, TriviaQA, PopQA, WebQuestions.</sub> |
|2023/10/23 | [Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy](https://arxiv.org/pdf/2305.15294.pdf) | Zhihong Shao, Yeyun Gong, yelong shen, Minlie Huang, et. al.| Tsinghua University, Microsoft Research Asia| <details><summary><small>This paper presents ITER-RETGEN ...</small></summary><small>This work propose **ITER-RETGEN**, which iterates retrieval-augmented generation and generation-augmented retrieval. Besides, they find that exact match can significantly underestimate the performance of LLMs, and using LLMs to evaluate is more reliable.</small></details>| <sub>HotpotQA, 2WikiMHQA, MuSiQue, Feverous, StrategyQA</sub> |
|2023/10/08 | [Retrieval-Generation Synergy Augmented Large Language Models](https://arxiv.org/pdf/2310.05149.pdf) | Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, Bing Qin| Harbin Institute of Technology| <details><summary><small>This paper presents ITRG ...</small></summary><small>This work propose **ITRG**, which contains two steps: 1) generation augmented retrieval (GAR) to expand the query based on previous iteration to help retrieve, 2) retrieval augmented generation (RAG) to generate new document to answer questions based on retrieved documents.</small></details>| <sub>NQ, TriviaQA, 2WikiMHQA, HotpotQA</sub> |
|2023/6/22 | [Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions](https://arxiv.org/abs/2212.10509) <br>[[code](https://github.com/stonybrooknlp/ircot): ![](https://img.shields.io/github/stars/stonybrooknlp/ircot.svg?style=social)] | Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal| Stony Brook University, Allen Institute for AI| <details><summary><small>This paper presents IRCoT ...</small></summary><small>This work propose **IRCoT**, which interleaves CoT generation and retrieval steps to guid the retrieval by CoT and vice-versa. Two steps: 1) reason step generates next CoT sentence based on question, retrieved passage, and CoT sentences; 2) retrieval step retrieves K more passages based on the last CoT sentence.</small></details>| <sub>HotpotQA,2WikiMHQA, MuSiQue, and IIRC</sub> |
