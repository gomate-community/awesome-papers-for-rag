## Introduction

Coming soon ...



## 1. Retrieval Methods <a id="retrieval"></a>

### 1.1 Two-tower Retriever (Tuning Embeddings) <a id="tunembedding"></a>

| Date       | Title | Authors   | Orgnization | Abs    | Base | Dataset     |
|------------|-------------------|---------------|------------------------|-----------------|------------------|--------------|
|2024/12/17 <br> (ðŸŒŸðŸŒŸðŸŒŸðŸŒŸðŸŒŸ)| [LLMs are Also Effective Embedding Models: An In-depth Overview](https://arxiv.org/pdf/2412.12591) | Chongyang Tao, Tao Shen, Shen Gao, et. al.| Beihang University, Tencent|<details><summary><small>**Survey**</small></summary><small>...</small></details>| -| <sub>-</sub>|
|2024/8/29  <br> (ðŸŒŸðŸŒŸðŸŒŸ)| [Conan-embedding: General Text Embedding with More and Better Negative Samples](https://arxiv.org/pdf/2408.15710) | Shiyu Li, Yang Tang, Shi-Zhen Chen, et. al.| Peking University, Tencent|<details><summary><small>This paper presents **conan-embedding** ...</small></summary><small>This work present **Conan-embedding**, which maximizes the utilization of more and higher-quality negative examples.</small></details>| <sub>MTEB</sub>|
|2024/4/9|[LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders](https://arxiv.org/pdf/2404.05961) <br>[[code](https://github.com/McGill-NLP/llm2vec): ![](https://img.shields.io/github/stars/McGill-NLP/llm2vec.svg?style=social)] |Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, et. al.|McGill University, Meta|<details><summary><small>This paper presents **LLM2Vec** ...</small></summary><small>This work introduces **LLM2Vec**, an unsupervised method to transform decoder-only large language models (LLMs) into powerful text encoders. The approach comprises three steps: (1) enabling bidirectional attention, (2) masked next token prediction, and (3) unsupervised contrastive learning. Applied to models ranging from 1.3B to 8B parameters, LLM2Vec achieves state-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB) among models trained solely on publicly available data. The method is parameter-efficient and does not require expensive adaptation or synthetic data.</small></details>|<sub>MTEB</sub>|
|2024/3/29 <br> (ðŸŒŸðŸŒŸðŸŒŸ)| [Gecko: Versatile Text Embeddings Distilled from Large Language Models](https://arxiv.org/pdf/2403.20327) | Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, et. al.| Google Deepmind|<details><summary><small>This paper presents **Gecko** ...</small></summary><small>This work present **Gecko**, a compact and versatile text embedding modle, which employs a two-stage distillation process by generating data and refining data quality based on large language models</small></details>| <sub>MTEB</sub>|
|2024/2/23  <br> (ðŸŒŸðŸŒŸðŸŒŸ)| [Repetition Improves Language Model Embeddings](https://arxiv.org/pdf/2402.15449.pdf) <br>[[code](https://github.com/jakespringer/echo-embeddings): ![](https://img.shields.io/github/stars/jakespringer/echo-embeddings.svg?style=social)] |Jacob Mitchell Springer, Suhas Kotha, Daniel Fried, et. al.| CMU| <details><summary><small>This paper presents **echo embeddings** ...</small></summary><small>This work present "**echo embeddings**", in which they repeat the input twice in context and extract embeddings from the second occurrence (i.e., repetition captures bidirectional information)</small></details>| <sub>MTEB</sub>|
|2024/2/15  <br> (ðŸŒŸðŸŒŸðŸŒŸ)| [Generative Representational Instruction Tuning](https://arxiv.org/abs/2402.09906.pdf), <br>[[code](https://github.com/ContextualAI/gritlm): ![](https://img.shields.io/github/stars/ContextualAI/gritlm.svg?style=social)]|Niklas Muennighoff, Hongjin Su, Liang Wang, et. al. | Contextual AI, The University of Hong Kong, Microsoft|<details><summary><small>This paper presents **GRIT** ...</small></summary><small>This work introduces generative representational instruction tuning (**GRIT**) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions</small></details>| <sub>MTEB</sub>|
|2024/2/10  <br> (ðŸŒŸðŸŒŸðŸŒŸðŸŒŸ)| [BGE M3-Embedding: Multi-Lingual, Multi-Functionality,Multi-Granularity Text Embeddings Through Self-Knowledge Distillation](https://arxiv.org/abs/2402.03216.pdf)|Liang Wang, Nan Yang, Xiaolong Huang, et. al.|BAAI, USTC|<details><summary><small>This paper presents **M3-embedding** ...</small></summary><small>This work present a new embedding model, called **M3-Embedding**, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity</small></details>| <sub>MIRACL, MKQA, MLDR</sub>|
|2024/1/19  <br> (ðŸŒŸðŸŒŸ)| [Improving Text Embeddings with Large Language Models](https://arxiv.org/abs/2401.00368.pdf)| Jianlv Chen, Shitao Xiao, Peitian Zhang, et. al.|Microsoft|<details><summary><small>This paper presents ...</small></summary><small>This work introduce a novel and simple method for obtaining high-quality text embeddings using only synthetic data and less than 1k training steps.</small></details>| <sub> BEIR, MTEB</sub>|
|2023/10/25 <br> (ðŸŒŸðŸŒŸðŸŒŸ)|[Retrieve Anything To Augment Large Language Models](https://arxiv.org/abs/2310.07554)<br>[[code](https://github.com/FlagOpen/FlagEmbedding): ![](https://img.shields.io/github/stars/FlagOpen/FlagEmbedding.svg?style=social)]|AutPeitian Zhang, Shitao Xiao, Zheng Liu, et. al.|BAAI, Renmin Univeristy of China, University of Montreal|<details><summary><small>This paper presents **LLM-Embedder** ...</small></summary><small>This work present a novel approach, the **LLM-Embedder**, which comprehensively supports the diverse retrieval augmentation needs of LLMs with one unified embedding model.</small></details>| <sub>MMLU, PopQA</sub>|
|2023/8/7|[Towards General Text Embeddings with Multi-stage Contrastive Learning](https://arxiv.org/pdf/2308.03281)|Zehan Li, Xin Zhang, Yanzhao Zhang, et. al.|Alibaba|<details><summary><small>This paper presents **GTE** ...</small></summary><small>This work introduces **GTE** (General Text Embeddings), a model trained using a multi-stage contrastive learning framework. The training involves large-scale unsupervised pre-training followed by supervised fine-tuning across diverse datasets. Despite its relatively modest parameter count of 110M, GTE<sub>base</sub> outperforms larger models and even surpasses the performance of OpenAI's black-box embedding API on the Massive Text Embedding Benchmark (MTEB). Notably, GTE also demonstrates strong capabilities in code retrieval tasks by treating code as text, achieving superior results without additional fine-tuning on specific programming languages.</small></details>|<sub>MTEB, BEIR, code retrieval</sub>|
|2023/5/30 <br> (ðŸŒŸðŸŒŸðŸŒŸðŸŒŸ)| [One Embedder, Any Task: Instruction-Finetuned Text Embeddings](https://arxiv.org/abs/2212.09741)<br>[[code](https://github.com/xlang-ai/instructor-embedding): ![](https://img.shields.io/github/stars/xlang-ai/instructor-embedding.svg?style=social)]|Hongjin Su, Weijia Shi, Jungo Kasai, et. al.|University of Hong Kong, University of Washingtong, Meta AI, Allen Institute for AI|<details><summary><small>This paper presents **INSTRUCTOR** ...</small></summary><small>This work introduce **INSTRUCTOR**, a new method for computing text embeddings given task instructions: every text input is embedded together with instructions explaining the use case (e.g., task and domain descriptions).</small></details>|GTR| <sub>MTEB</sub>|
|2022/9/23 <br> (ðŸŒŸðŸŒŸðŸŒŸ)| [Promptagator: Few-shot Dense Retrieval From 8 Examples](https://arxiv.org/abs/2209.11755)|Zhuyun Dai, Vincent Y. Zhao, Ji Ma, et. al.|Google Research|<details><summary><small>This paper presents **Promptagator** ...</small></summary><small>This work propose Prompt-base Query Generation for Retriever (**Promptagator**), which leverages large language models (LLM) as a few-shot query generator, and creates task-specific retrievers based on the generated data.</small></details>|FLAN| <sub>BEIR</sub>|


### 1.2 LLM-based Retriever<a id="aligning"></a>

| Date       | Title | Authors   | Orgnization | Abs    | Dataset                                                                                           |
|------------|-----------------------------------------------------------------------------------------------------------------|------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|
|2025/04/29 <br> (ðŸŒŸ)|[ReasonIR: Training Retrievers for Reasoning Tasks](https://arxiv.org/abs/2504.20595) <br>[[code](https://github.com/facebookresearch/ReasonIR): ![](https://img.shields.io/github/stars/facebookresearch/ReasonIR.svg?style=social)]| Rulin Shao, Rui Qiao, Varsha Kishore, et al.|FAIR at Meta|<details><summary><small>**ReasonIR**: pointwise, finetune, LLaMA3.1-8B.</small></summary><small>This work includes ** ReasonIR**, which trained on ReasonIR-Synthesizer data (1,383,877 public samples, 244,970 varied-length samples, 100,521 hard samples).</small></details>|<sub>BRIGHT, MMLU, GPQA</sub>|
|2023/12/24 <br> (ðŸŒŸðŸŒŸðŸŒŸðŸŒŸ)|[Making Large Language Models A Better Foundation For Dense Retrieval](https://arxiv.org/pdf/2312.15503.pdf) <br>[[code](https://github.com/FlagOpen/FlagEmbedding): ![](https://img.shields.io/github/stars/FlagOpen/FlagEmbedding.svg?style=social)]| Chaofan Li, Zheng Liu, Shitao Xiao, et. al.|BAAI, BUPT|<details><summary><small>This paper presents **LLaRA** ...</small></summary><small>This work includes **LLaRA** (LLM Adapted for dense RetriAl), which introduce two pretext training tasks EBAE (Embedding-Based Auto-Encoding) and EBAR (Embedding-Based Auto-Regression) to improve LLaMA for dense retrieval.</small></details>|<sub>MS MARCO passage&document, BEIR</sub>|


<!--
|2023/12/16| [UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation](https://arxiv.org/abs/2303.08518.pdf)<br>[[code](https://github.com/microsoft/LMOps): ![](https://img.shields.io/github/stars/microsoft/LMOps.svg?style=social) |Daixuan Cheng, Shaohan Huang, Junyu Bi, et. al.|Microsoft|<details><summary><small>This paper presents UPRISE ...</small></summary><small>This work propose  UPRISE (Universal Prompt Retrieval for Improving zero-Shot Evaluation), which tunes a lightweight and versatile retriever that automatically retrieves prompts for a given zero-shot task input.</small></details>|
-->

### 1.3 LLM-guided Retriever<a id="llm-guided-retriever"></a>

| Date       | Title | Authors   | Orgnization | Abs    | Dataset                                                                                           |
|------------|-----------------------------------------------------------------------------------------------------------------|------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|
|2024/03/27  <br> (ðŸŒŸðŸŒŸ)| [LLatrieval: LLM-Verified Retrieval for Verifiable Generation](https://arxiv.org/pdf/2311.07838.pdf)<br>[[code](https://github.com/BeastyZ/LLM-Verified-Retrieval): ![](https://img.shields.io/github/stars/BeastyZ/LLM-Verified-Retrieval.svg?style=social))]| Xiaonan Li, Changtai Zhu, Linyang Li, et. al.| Fudan University | <details><summary><small>This paper presents LLatrieval ...</small></summary><small>This work proposes **LLatrieval** (LLM-verified Retrieval), where the LLM iteratively provides feedback to the retrieval through verify-update iterations. 1) **Retrieval Verification** is implemented by prompting LLM to give binary label, and 2) **Retrieval Update** takes LLM to progressively scan the document candidates returned by the retriever and selects the supporting documents.</small></details>|<sub>ALCE</sub>|
|2024/3/15  <br> (ðŸŒŸðŸŒŸðŸŒŸ)| [DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models](https://arxiv.org/abs/2403.10081) <br>[[code](https://github.com/oneal2000/DRAGIN): ![](https://img.shields.io/github/stars/oneal2000/DRAGIN.svg?style=social)] | Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, Yiqun Liu|  Tsinghua University, Beijing Institute of Technology| <details><summary><small>**DRAGIN** ...</small></summary><small>This work introduce a new framework, **DRAGIN**, i.e., Dynamic Retrieval Augmented Generation based on the real-time Information Needs of LLMs. This framework is specifically designed to make decisions on when and what to retrieve based on the LLMâ€™s real-time information needs during the text generation process.</small></details>|<sub>2WikiMHQA, HotpotQA, IIRC, StrategyQA</sub>|
|2023/5/26  <br> (ðŸŒŸðŸŒŸ)| [Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In](https://arxiv.org/abs/2305.17331.pdf) <br> [[code](https://github.com/OpenMatch/Augmentation-Adapted-Retriever): ![](https://img.shields.io/github/stars/OpenMatch/Augmentation-Adapted-Retriever.svg?style=social)] |Zichun Yu, Chenyan Xiong, Shi Yu, Zhiyuan Liu|Tsinghua University, Microsoft|<details><summary><small>This paper presents AAR ...</small></summary><small>This work introduce augmentation-adapted retriever (AAR), which takes a black-box LLM to score positive documents (so called LLM-preferred signals) for fine-tuning a pre-trained retriever.</small></details>|<sub>MMLU, PopQA</sub>|


### 1.4 Structured Retriever <a id="structured-retriever"></a>

| Date  | Title | Authors   | Orgnization | Abs    | Dataset   |
|------------|-----------------------------------------------------------------------------------------------------------------|------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|
|2025/05/20 <br> (ðŸŒŸðŸŒŸðŸŒŸ)|[MacRAG: Compress, Slice, and Scale-up for Multi-scale Adaptive Context RAG](https://arxiv.org/pdf/2505.06569) [[code](https://github.com/Leezekun/MacRAG): ![](https://img.shields.io/github/stars/Leezekun/MacRAG.svg?style=social)]|Woosang Lim, Zekun Li, Gyuwan Kim, et al.|POSCO HOLDINGS|<details><summary><small>**MacRAG**: Multi-scale Adaptive Context RAG.</small></summary><small> The *MacRAG** is a hierarchical RAG framework that compresses and partitions documents into coarse-to-fine granularities, then adaptively merges relevant contexts through real-time chunk- and document-level expansions.</small></details>|<sub> HotpotQA, 2WikiMultihopQA, Musiqu</sub>|
|2025/04/04 <br> (ðŸŒŸ)|[EnrichIndex: Using LLMs to Enrich Retrieval Indices Offline](https://arxiv.org/pdf/2504.03598) [[code](https://github.com/peterbaile/enrichindex): ![](https://img.shields.io/github/stars/peterbaile/enrichindex.svg?style=social)]|Peter Baile Chen, Tomer Wolfson, Michael Cafarella, Dan Roth|MIT, University of Pennsylvania|<details><summary><small>**EnrichIndex**: zeroshot, GPT-4o-mini.</small></summary><small> The *EnrichIndex** uses off-the-shelf GPT-4o-mini to enrich each object with three additional representations during indexing phase: 1) its purpose, 2) a summary, and 3) question-answer pairs. The  final score of each query-object is a weighted sum of the similarity scores between q and each representation.</small></details>|<sub>BRIGHT, Spider2, Beaver, Fiben</sub>|
|2024/01/31 <br> (ðŸŒŸðŸŒŸðŸŒŸðŸŒŸ)|[RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval](https://arxiv.org/abs/2401.18059) [[code](https://github.com/parthsarthi03/raptor): ![](https://img.shields.io/github/stars/parthsarthi03/raptor.svg?style=social)]|Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning|Stanford|<details><summary><small>**RAPTOR** ...</small></summary><small>We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of summarization from the bottom up. At inference time, our RAPTOR model retrieves from this tree, integrating information across lengthy documents at different levels of abstraction.  On question-answering tasks that involve complex, multi-step reasoning, we show state-of-the-art results; for example, by coupling RAPTOR retrieval with the use of GPT-4, we can improve the best performance on the QuALITY benchmark by 20% in absolute accuracy.</small></details>|<sub>NarrativeQA, QASPER, QuALITY</sub>|


## 2. ReRanking Methods <a id="rerank"></a>
### 2.1 LLM for Ranking <a id="llmrank"></a>
These methods try to leverage LLMs to directly rerank documents, usually in listwise setting. The core of these methods lies on how to **divide** the list into small local groups, and then how to **aggregate** local results into global ranking. 

| Date       | Title | Authors   | Orgnization | Abs    | Dataset                                                                                           |
|------------|-----------------------------------------------------------------------------------------------------------------|------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|
|2025/02/06 <br> (ðŸŒŸðŸŒŸðŸŒŸ)|[TourRank: Utilizing Large Language Models for Documents Ranking with a Tournament-Inspired Strategy](https://arxiv.org/pdf/2406.11678) [[code](https://github.com/chenyiqun/TourRank): ![](https://img.shields.io/github/stars/chenyiqun/TourRank.svg?style=social)]|Yiqun Chen, Qi Liu, Yi Zhang, et al. | Renmin University & Baidu|<details><summary><small>**TourRank**: listwise, zeroshot, GPT-3.5.</small></summary><small> This paper propose a zero-shot document ranking method called **TourRank**. It first groups candidate documents and prompt LLM to select the most relevant one in each group. It also designes a points systems to assign different points to each document based on its ranking in each round tournament. </small></details>|<sub>BEIR, TREC-DL</sub>|
|2024/06/21 <br> (ðŸŒŸðŸŒŸðŸŒŸðŸŒŸ)|[FIRST: Faster Improved Listwise Reranking with Single Token Decoding](https://arxiv.org/pdf/2406.15657) [[code](https://github.com/gangiswag/llm-reranker): ![](https://img.shields.io/github/stars/gangiswag/llm-reranker.svg?style=social)]|Revanth Gangi Reddy, JaeHyeok Doo, Yifei Xu, et al.| UIUC|<details><summary><small>**FIRST**: listwise, finetune, Zephyr-Î².</small></summary><small> This work introduces **FIRST**, which simply extracts the output logits of candidate identifier tokens while generating the first identifier y1 and returns the passage ranking in the order of decreasing logit values. It uses 40k GPT-4 labeled instances (5k queries from MS MARCO) from *Rankzephyr* for finetunning LLM reranks.</small></details>|<sub>BEIR</sub>|
|2024/05/30 <br> (ðŸŒŸðŸŒŸðŸŒŸ)|[A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models](https://arxiv.org/pdf/2310.09497) [[code](https://github.com/ielab/llm-rankers): ![](https://img.shields.io/github/stars/ielab/llm-rankers.svg?style=social)]|Shengyao Zhuang, Honglei Zhuang, Bevan Koopman, Guido Zuccon | CSIRO, Australia|<details><summary><small>**Setwise Prompting**: setwise, zeroshot, Flan-t5.</small></summary><small> This work focus on LLM-based zero-shot document ranking, and introduce an **Setwise** prompting strategy. It instructs LLMs to select the most relevant document to the query from a set of candidate documents.</small></details>|<sub>BEIR, TREC-DL</sub>|
|2024/05/23 <br> (ðŸŒŸðŸŒŸðŸŒŸ)|[Top-Down Partitioning for Efficient List-Wise Ranking](https://arxiv.org/pdf/2405.14589v1) [[code](https://github.com/Parry-Parry/TDPart): ![](https://img.shields.io/github/stars/Parry-Parry/TDPart.svg?style=social)]|Andrew Parry, Sean MacAvaney, Debasis Ganguly|University of Glasgow|<details><summary><small>**TDPart**: listwise, zeroshot, GPT-3.5.</small></summary><small> This work partitions a ranking to depth k and processes documents to-down. At each round, it selects a pivot document, and compared it with documents from each group. Those winner documents of each group are collected as the input of next round.</small></details>|<sub>MSMARCO, TREC-DL, BEIR</sub>|
|2024/03/28 <br> (ðŸŒŸðŸŒŸ)| [Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting](https://arxiv.org/pdf/2306.17563.pdf) | Zhen Qin, Rolf Jagerman, Kai Hui, et al.|Google Research|<details><summary><small>**PRP**: pairwise, zeroshot, FLAN-T5(3B,11B,20B).</small></summary><small>This work introduces **Pairwise Ranking Prompting (PRP)** for ranking with LLMs. Three variant of PRP: 1) all pair comparisons $O(N^2)$, 2) Sorting-based, i.e., Heapsort, $O(N\times \log(N))$, 3) Sliding window, i.e., Bubble Sort for Top-K, $O(N)$.</small></details>| <sub>TREC-DL2019&2020, BEIR</sub>|
|2024/02/20 <br> (ðŸŒŸðŸŒŸ)| [Bridging the Preference Gap between Retrievers and LLMs](https://arxiv.org/pdf/2401.06954) | Zixuan Ke, Weize Kong, Cheng Li, et al.|Google Research|<details><summary><small>**BGM**: listwise, finetune, T5-XXL(11B).</small></summary><small>This work trains a seq2seq bridge model (directly generates passage IDs) to jointly accomplish reranking and selection, adapting the retrieved information to be LLM-friendly. It employs a SL and RL training scheme to optimize the adapation process.</small></details>| <sub>NQ, HotpotQA, Avocado Email, Amazon Book</sub>|
|2023/12/05 <br> (ðŸŒŸðŸŒŸ)|[RankZephyr: Effective and Robust Zero-Shot Listwise Reranking is a Breeze!](https://arxiv.org/abs/2312.02724) [[code](https://github.com/castorini/rank_llm): ![](https://img.shields.io/github/stars/castorini/rank_llm.svg?style=social)]|Ronak Pradeep, Sahel Sharifymoghaddam, Jimmy Lin| University of Waterloo|<details><summary><small>**RankZephyr**: listwise, finetune, Zephyr-Î².</small></summary><small> The **RankZephyr** is trained with two stages: the first stage is trained on 100k queries from msmarco v1, where 20 candidate documents of each query are ranked by RankGPT3.5. The second stage is trained with less than 5k queries labeled by RankGPT4. When ranking the top-100 candidates, it employed a sliding window akin to RankGPT and RankVicuna.</small></details>|<sub>BEIR, TREC-DL 19&20, 21, 22</sub>|
|2023/10/21  <br> (ðŸŒŸðŸŒŸ)| [Beyond Yes and No: Improving Zero-Shot LLM Rankers via Scoring Fine-Grained Relevance Labels](https://arxiv.org/pdf/2310.14122.pdf) | Honglei Zhuang, Zhen Qin, Kai Hui, Junru Wu, et al. |Google Research |<details><summary><small>**RG-S**: pointwise, zeroshot, FLAN PaLM2S.</small></summary><small>This work proposes to incorporate **fine-grained relevance labels (not-relevant/somewhat-relevant/highly-relevant)** into the prompt for point-wise LLM rankers. The method is called **RG-S**, which is rating scale 0-k Relevance Generation (RG-S(0,k)). It directly prompts LLM to rate the relevance for each q-d pair using a scale from 0 to k.</small></details>| <sub>BEIR</sub>|
|2023/10/20 <br> (ðŸŒŸðŸŒŸ)| [Open-source Large Language Models are Strong Zero-shot Query Likelihood Models for Document Ranking](https://arxiv.org/pdf/2310.13243.pdf) [[code](https://github.com/ielab/llm-qlm): ![](https://img.shields.io/github/stars/ielab/llm-qlm.svg?style=social)]| Shengyao Zhuang, Bing Liu, Bevan Koopman, Guido Zuccon |CSIRO |<details><summary><small>**LLM-QLM**: pointwise,zeroshot,LLaMA(7B).</small></summary><small>This work finds that open-source LLMs can be effective point-wise rankers by asking them to *generate the query given the content of a document*.</small></details>| <sub>BEIR</sub>|
|2023/10/12 <br> (ðŸŒŸðŸŒŸðŸŒŸ)|[Fine-Tuning LLaMA for Multi-Stage Text Retrieval](https://arxiv.org/abs/2310.08319) ]|Xueguang Ma, Liang Wang, Nan Yang, et al.| University of Waterloo|<details><summary><small>**RepLLaMA and RankLLaMA**: pointwise, finetune, LLaMA.</small></summary><small> This paper introduces **RepLLaMA** and **RankLLaMA**, which finetunes LLaMA model as dense retriever and pointwise reranker using MS MARCO datasets.</small></details>|<sub>MS MARCO passage/document, BEIR</sub>|
|2023/09/26 <br> (ðŸŒŸðŸŒŸðŸŒŸ)|[RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models](https://arxiv.org/abs/2309.15088) [[code](https://github.com/castorini/rank_llm): ![](https://img.shields.io/github/stars/castorini/rank_llm.svg?style=social)]|Ronak Pradeep, Sahel Sharifymoghaddam, Jimmy Lin| University of Waterloo|<details><summary><small>**RankVicuna**: listwise, finetune, Vicuna.</small></summary><small> The **RankVicuna** is trained on the ranked list generated by RankGPT3.5 for 100k queries from msmarco v1. Each query has 20 candidates provided by BM25. They filtered noises: 1) malformed generations, where 12% outputs incorrect list formatting. 2) shuffle the order of candidate documents as data augmentation.</small></details>|<sub> TREC-DL 19&20</sub>|
|2023/04/19 <br> (ðŸŒŸðŸŒŸðŸŒŸðŸŒŸ)|[Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents](https://arxiv.org/abs/2304.09542) [[code](https://github.com/sunnweiwei/RankGPT): ![](https://img.shields.io/github/stars/sunnweiwei/RankGPT.svg?style=social)]|Weiwei Sun, Lingyong Yan, Xinyu Ma, et al., | Shandong University|<details><summary><small>**RankGPT**: listwise, zeroshot, GPT-3.5.</small></summary><small> This work first investigates prompting ChatGPT on passage re-ranking tasks, and find LLMs show limited performance. It proposes an instructional permutation generation method, and use sliding window to address context length limitation of LLMs.</small></details>|<sub>BEIR, TREC-DL</sub>|

<!--- 
|2023/5/1| [Say Goodbye to Irrelevant Search Results: Cohere Rerank Is Here](https://txt.cohere.com/rerank)| NILS REIMERS, SYLVIE SHI, LUCAS FAYOUX, ELLIOTT CHOI| Cohere| <details><summary><small>This work presents Cohere Rerank ...</small></summary><small>This work proposes Cohere Rerank,which can provide a powerful semantic boost to the search quality of any keyword or vector search system without requiring any overhaul or replacement.</small></details>| <sub>MIRACL, TREC-DL 19&20, NQ</sub>|
--->

### 2.2 Reasoning for Ranking (fine-tuned LLM) <a id="reasonrank"></a>

These methods try to improve the reasoning ability of LLMs in ranking document. Usually, they first **construct** a large-scale dataset which contains the "reasoing chain" in each sample. Then they **finetune** LLM on the dataset to obtain reason-enhanced reranking model.

| Date       | Title | Authors   | Orgnization | Abs    | Dataset                                                                                           |
|------------|-----------------------------------------------------------------------------------------------------------------|------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|
|2025/08/25 <br> (ðŸŒŸðŸŒŸðŸŒŸ) |[DIVER: A Multi-Stage Approach for Reasoning-intensive Information Retrieval](https://arxiv.org/pdf/2508.07995?) [[code](https://github.com/AQ-MedAI/Diver): ![](https://img.shields.io/github/stars/AQ-MedAI/Diver.svg?style=social)]|Meixiu Long, Duolin Sun, Dan Yang, et al.|Ant Group|<details><summary><small>**DIVER**: RL, Qwen2.5.</small></summary><small> This work proposes **DIVER**, a retrieval pipeline designed for reasoning-intensive information retrieval. It consists of four components: **DIVER-DChunk**, **DIVER-QExpand**, **DIVER-Retriever**, and **DIVER-Rerank**.</small></details>|<sub>BRIGHT</sub>|
|2025/08/22 <br> (ðŸŒŸðŸŒŸ) |[ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability](https://arxiv.org/pdf/2508.07050) [[code](https://github.com/8421BCD/ReasonRank): ![](https://img.shields.io/github/stars/8421BCD/ReasonRank.svg?style=social)]|Wenhan Liu, Xinyu Ma, Weiwei Sun, et al.|Renming University|<details><summary><small>**ReasonRank**: RL, Qwen2.5.</small></summary><small> This work proposes **ReasonRank**, a reasoning-intensive passage reranker. It consists of a ranking data synthesis framework, and a two-state training framework (i.e., a cold-start SFT and a multi-view ranking based RL).</small></details>|<sub>BRIGHT, R2MED</sub>|
|2025/05/20 <br> (ðŸŒŸðŸŒŸ) |[Rank-K: Test-Time Reasoning for Listwise Reranking](https://arxiv.org/pdf/2505.14432) [[code](https://github.com/hltcoe/rank-k): ![](https://img.shields.io/github/stars/hltcoe/rank-k.svg?style=social)]|Eugene Yang, Andrew Yates, Kathryn Ricci, et al.|Johns Hopkins University|<details><summary><small>**Rank-k**: RL, Qwen2.5.</small></summary><small> This work proposes **Rank-k**, a listwise passage reranking model that leverages the reasoning capability of the reasoning language model at query time that provides test time scalability to serve hard queries.</small></details>|<sub>DL19, DL20, NeuCLIR22, NeuCLIR24, BRIGHT</sub>|
|2025/05/07 <br> (ðŸŒŸðŸŒŸ) |[ZEROSEARCH: Incentivize the Search Capability of LLMs without Searching](https://arxiv.org/pdf/2505.04588) [[code](https://github.com/Alibaba-NLP/ZeroSearch): ![](https://img.shields.io/github/stars/Alibaba-NLP/ZeroSearch.svg?style=social)]|Hao Sun, Zile Qiao, Jiayan Guo, et al.|Tongyi Lab, Alibaba Group|<details><summary><small>**ZeroSearch**: RL, Qwen2.5.</small></summary><small> This work proposes **ZeroSearch**, a RL framework that incentivizes the search capability of LLMs. It transforms LLM into a retrieval module by supervised fine-tuning.It introduces a curriculum rollout mechanism to progressively elicit model's reasoning ability by exposing it to increasingly challenging retrieval scenarios. The rollout trajectory contains [<think><search><information><answer>].</small></details>|<sub>NQ, TrivaiQA, PopQA, HotpotQA, 2WikiMultiHopQA, Musique, Bamboogle</sub>|
|2025/03/27 <br> (ðŸŒŸðŸŒŸ) |[ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning](https://arxiv.org/pdf/2503.19470) [[code](https://github.com/Agent-RL/ReCall): ![](https://img.shields.io/github/stars/Agent-RL/ReCall.svg?style=social)]|Mingyang Chen, Tianpeng Li, Haoze Sun, et al.| Baichuan Inc.|<details><summary><small>**ReSearch**: RL, Qwen2.5.</small></summary><small> The **ReSearch** trains LLMs to reason with search via RL without any supervised data. The rollout trajectory contains [<think><search><result><answer>].</small></details>|<sub>HotpotQA, 2WikiMultiHopQA, Musique, Bamboogle</sub>|
|2025/03/18 <br> (ðŸŒŸðŸŒŸ) |[R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning](https://arxiv.org/pdf/2503.05592) [[code](https://github.com/RUCAIBox/R1-Searcher): ![](https://img.shields.io/github/stars/RUCAIBox/R1-Searcher.svg?style=social)]|Huatong Song, Jinhao Jiang, Yingqian Min, et al.|Renmin University|<details><summary><small>**R1-Search**: RL, Qwen2.5.</small></summary><small> The **R1-Search** utilizes a two-stage, outcome-based training strategy. The first stage uses retrieve-reward to incentivize the model to conduct retrieval operations. The second stage introduce answer-reward to encourage model to learn to use external retrieval system to solve questions. The rollout trajectory contains [<think><query><document><answer>].</small></details>|<sub>HotpotQA, 2WikiMultiHopQA, Musique, Bamboogle</sub>|
|2025/03/12 <br> (ðŸŒŸðŸŒŸ) |[Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning](https://arxiv.org/abs/2503.09516) [[code](https://github.com/PeterGriffinJin/Search-R1): ![](https://img.shields.io/github/stars/PeterGriffinJin/Search-R1.svg?style=social)]|Bowen Jin, Hansi Zeng, Zhenrui Yue, et al.|UIUC, Google|<details><summary><small>**Search-R1**: RL, Qwen2.5.</small></summary><small> The **Search-R1** optimizes LLM to [search/reason/answer] within multi-turn search interactions, leveraging retrival token masking for stable RL training and a smiple outcome-based reward function.The rollout trajectory contains [<think><search><information><answer>].</small></details>|<sub>NQ, TrivaiQA, PopQA, HotpotQA, 2WikiMultiHopQA, Musique, Bamboogle</sub>|
|2025/02/25 <br> (ðŸŒŸðŸŒŸ)|[Rank1: Test-Time Compute for Reranking in Information Retrieval](https://arxiv.org/pdf/2502.18418) [[code](https://github.com/orionw/rank1): ![](https://img.shields.io/github/stars/orionw/rank1.svg?style=social)]|Orion Weller, Kathryn Ricci Eugene Yang, Andrew Yates, et al.|Johns Hopkins University|<details><summary><small>**Rank1**: pointwise, finetune, Qwen2.5.</small></summary><small> This work sample 635,000  examples of R1's reasoning on the MS MARCO dataset. It then finetunes the qwen 2.5 model on these reasoning chains and find they show remarkable reasoning capabilities.</small></details>|<sub>BRIGHT</sub>|
|2025/01/09 <br> (ðŸŒŸðŸŒŸðŸŒŸ) |[Search-o1: Agentic Search-Enhanced Large Reasoning Models](https://arxiv.org/pdf/2501.05366) [[code](https://github.com/sunnynexus/Search-o1): ![](https://img.shields.io/github/stars/sunnynexus/Search-o1.svg?style=social)]|Xiaoxi Li, Guanting Dong, Jiajie Jin, et al.|Renmin University|<details><summary><small>**Search-o1**: Zeroshot, QwQ-32B.</small></summary><small> The **Search-o1** combines the reasoning process with an agentic RAG mechanism and a knowledge refinement module. The reason-in-document module operates independently from the main reasoning chain, which conducts a thorough analysis of retrieved documents and produces refined information.</small></details>|<sub>GPQA, MATH500, AMC2023, AIME2024, LiveCodeBench, NQ, TriviaQA, HotpotQA, 2WikiMultihopQA, MuSiQue, Bamboogle</sub>|
|2024/10/31 <br> (ðŸŒŸ)|[JudgeRank: Leveraging Large Language Models for Reasoning-Intensive Reranking](https://arxiv.org/pdf/2411.00142)|Tong Niu, Shafiq Joty, Ye Liu, et al.| Salesforce AI Research|<details><summary><small>**JudgeRank**: pointwise, zeroshot, Llama-3.1.</small></summary><small> The **JudgeRank** estimate the relevance of query-document pairs following three steps: 1) query analysis to identify the core problem (query expansion), 2) document analysis to **extract** a query-aware summary, 3) relevance judgement to provide score based on the probability of "yes" and "no".</small></details>|<sub>BRIGHT, BEIR</sub>|

## 3. Analysis about Retrieval<a id="analysis"></a>

| Date       | Title | Authors   | Orgnization | Abs    | Dataset                                                                                           |
|------------|-----------------------------------------------------------------------------------------------------------------|------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|
|2024/7/14 <br> (ðŸŒŸðŸŒŸðŸŒŸðŸŒŸ)| [The Power of Noise: Redefining Retrieval for RAG Systems](https://arxiv.org/abs/2401.14887) <br>[[code](https://github.com/florin-git/The-Power-of-Noise): ![](https://img.shields.io/github/stars/florin-git/The-Power-of-Noise.svg?style=social)] | F. Cuconasu, G. Trappolini, F. Siciliano, etl. al.|  Sapienza University of Rome| <details><summary><small>This paper studies noise passages ...</small></summary><small>This paper studied the impact of noise passages in RAG, and found that adding random documents in the prompt improves the LLM accuracy by up to 35% on the NQ dataset.</small></details>|<sub>NQ-open (subset of NQ)</sub>|

