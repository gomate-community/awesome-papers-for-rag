
## Introduction

Coming soon ...

## 1. Instruction Fine-tuning <a id="ift"></a>

### 1.1 Knowledge Enhance <a id="otm"></a>

| Date       | Title | Authors   | Orgnization | Abs    | Dataset                                                                                           |
|------------|-----------------------------------------------------------------------------------------------------------------|------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|
|2024/03/15 <br> (ğŸŒŸğŸŒŸğŸŒŸğŸŒŸ)| [RAFT: Adapting Language Model to Domain Specific RAG](https://arxiv.org/abs/2403.10131) [[code](https://github.com/ShishirPatil/gorilla): ![](https://img.shields.io/github/stars/ShishirPatil/gorilla.svg?style=social)]| Tianjun Zhang, Shishir G. Patil, Naman Jain, Sheng Shen, et al.|UC Berkeley|<details><summary><small>This paper presents *RAFT* ...</small></summary><small> RAFT leverages fine-tuning with question-answer pairs while referencing the documents in a simulated imperfect retrieval setting â€” thereby effectively preparing for the open-book exam setting. The RAFT is trained to answer the question (Q) from Document(s) (D) to generate answer (A), where A includes chain-of-thought reasoning.</small></details>| <sub>PubMed, HotpotQA, Gorilla</sub> |
|2023/5/18 <br> (ğŸŒŸğŸŒŸ)| [Augmented Large Language Models with Parametric Knowledge Guiding](https://arxiv.org/abs/2305.04757) | Ziyang Luo, Can Xu, Pu Zhao, et. al.,| Hong Kong Baptist University, Microsoft| <details><summary><small>This paper presents PKG ...</small></summary><small>This work propose Parametric Knowledge Guiding (**PKG**), which injects domain knowledge for LLaMa-7B via instruction fine-tuning to capture the necessary expertise. Then, the PKG is used to generage context for a given question as the background-augmented prompting for LLMs.</small></details>| <sub>FM2, NQ-Table, MedMC-QA, ScienceQA</sub>|


### 1.2 Attribution Enhance <a id="attribution"></a>

| Date       | Title | Authors   | Orgnization | Abs    | Dataset                                                                                           |
|------------|-----------------------------------------------------------------------------------------------------------------|------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|
|2025/02/13 <br> (ğŸŒŸğŸŒŸğŸŒŸğŸŒŸ) | [SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models](https://arxiv.org/pdf/2502.09604) | Yung-Sung Chuang, Benjamin Cohen-Wang, Shannon Zejiang Shen, Zhaofeng Wu, Hu Xu, Xi Victoria Lin, James Glass, Shang-Wen Li, Wen-tau Yih | Meta FAIR, MIT | <details><summary><small>This paper presents **SelfCite** ...</small></summary><small>SelfCite leverages a reward signal provided by the LLM itself through context ablation: If a citation is necessary, removing the cited text from the context should prevent the same response; if sufficient, retaining the cited text alone should preserve the same response. This reward can guide the inference-time best-of-N sampling strategy to improve citation quality significantly, as well as be used in preference optimization to directly fine-tune the models for generating better citations. </small></details>|<sub>LongBench-Cite</sub>|
|2024/12/19 <br> (ğŸŒŸğŸŒŸğŸŒŸ) | [VISA: Retrieval Augmented Generation with Visual Source Attribution](https://arxiv.org/pdf/2412.14457) | Xueguang Ma, Shengyao Zhuang, Bevan Koopman, Guido Zuccon, Wenhu Chen, Jimmy Lin | University of Waterloo, CSIR, University of Queensland | <details><summary><small>This paper presents **VISA** ...</small></summary><small>This work proposes Retrieval-Augmented Generation with Visual Source Attribution (VISA), which processes single or multiple retrieved document images, and generates an answer as well as the bounding box of the relevant region within the evidence document. They curated two datasets: Wiki-VISA and Paper-VISA, to fine-tune the QWen2-VL-72B</small></details>|<sub>Wiki-VISA, Paper-VISA</sub>|
|2024/09/10 <br> (ğŸŒŸğŸŒŸğŸŒŸ) | [LongCite: Enabling LLMs to Generate Fine-grained Citations in Long-context QA](https://arxiv.org/abs/2409.02897) [[code](https://github.com/THUDM/LongCite): ![](https://img.shields.io/github/stars/THUDM/LongCite.svg?style=social)] | Jiajie Zhang, Yushi Bai, Xin Lv, et. al.| Tsinghua University| <details><summary><small>This paper presents **LongCite** ...</small></summary><small>This work proposes CoF (abbr. for â€œCoarse to Fineâ€), that utilizes off-the-shelf LLMs to automatically construct long-context QA instances with precise sentence-level citations. CoF comprises four stages: (1) Starting with a long text material, CoF first invokes the LLM to produce a query and its answer through Self-Instruct. (2) CoF uses the answer to retrieve several chunks from the context, which are then fed into the LLM to incorporate coarse-grained chunk-level citations within the answer. (3) The LLM identifies relevant sentences from each cited chunk to produce fine-grained citations. (4) instances with an insufficient number of citations are discarded. </small></details>|<sub>LongBench-Cite</sub>|
|2024/08/20 <br> (ğŸŒŸğŸŒŸğŸŒŸğŸŒŸ) | [INSTRUCTRAG: Instructing Retrieval-Augmented Generation via Self-Synthesized Rationales](https://arxiv.org/pdf/2406.13629) [[code](https://github.com/weizhepei/InstructRAG): ![](https://img.shields.io/github/stars/weizhepei/InstructRAG.svg?style=social)] | Zhepei Wei, Wei-Lin Chen, Yu Meng | University of Virginia | <details><summary><small>This paper presents **InstructRAG** ...</small></summary><small>This work proposes InstructRAG to generate rationales along with the answer, enhancing both the generation accuracy and trustworthiness. It first prompt an instruction-tuned LLM (rational generator) to synthesize rationales, which is to explain how to derive correct answer from noisy retrieved documents. Then, it guid the LM to learn explict denoising by leveraging these rationals as either in-context learning demonstrations or as supervised fine-tuning data.</small></details>|<sub>PopQA, TriviaQA, NQ, ASQA, 2WikiMHQA </sub>|
|2024/08/08 <br> (ğŸŒŸğŸŒŸğŸŒŸğŸŒŸ) | [Learning Fine-Grained Grounded Citations for Attributed Large Language Models](https://arxiv.org/abs/2408.04568) [[code](https://github.com/LuckyyySTA/Fine-grained-Attribution): ![](https://img.shields.io/github/stars/LuckyyySTA/Fine-grained-Attribution.svg?style=social)] | Lei Huang, Xiaocheng Feng, Weitao Ma, et. al. | Harbin Institute of Technology, Harbin | <details><summary><small>This paper presents **FRONT** ...</small></summary><small>This work introduces *FRONT*, a two-stage training framework designed to teach LLMs to generate Fine-gRained grOuNded ciTations, consisting of Grounding Guided Generation (G3) and Consistency-Aware Alignment (CAA). During the G3 stage, the LLM first selects supporting quotes from retrieved sources (grounding) and then conditions the generation process on them (generation). The CAA stage then utilizes preference optimization to further align the grounding and generation process by automatically constructing preference signals. </small></details>|<sub>ALCE(ASQA, ELI5, QAMPARI)</sub>|
|2024/07/01 <br> (ğŸŒŸğŸŒŸğŸŒŸ)  | [Ground Every Sentence: Improving Retrieval-Augmented LLMs with Interleaved Reference-Claim Generation](https://arxiv.org/pdf/2407.01796) | Sirui Xia, Xintao Wang, Jiaqing Liang, et al. | Fudan University, AntGroup | <details><summary><small>This paper presents **ReClaim** ...</small></summary><small>Contributions: 1) ReClaim alternately generates citations and answer sentences, to enable large models to generate answer with citations. 2) For ReClaim, they constructed a training dataset and fine-tuned the model using different approaches to improve its attribution capability. 3) Through multiple experiments, they demonstrated the effectiveness of the method in enhancing the modelâ€™s verifiability and credibility. </small></details>|<sub>ASQA, ELI5 </sub>|
|2024/03/27<br> (ğŸŒŸ)  | [Improving Attributed Text Generation of Large Language Models via Preference Learning](https://arxiv.org/pdf/2403.18381) [[code](https://github.com/HITsz-TMG/ATG-PO): ![](https://img.shields.io/github/stars/HITsz-TMG/ATG-PO.svg?style=social)] | Dongfang Li, Zetian Sun, Baotian Hu, et. al. | Harbin Institute of Technology (Shenzhen) | <details><summary><small>This paper presents APO ...</small></summary><small>This work conceptualize the attribution task for LLMs as preference learning and proposing an Automatic Preference Optimization (APO) framework. They assemble a curated dataset comprising 6,330 examples sourced and refined from existing datasets for posttraining. Beside, they further propose an automatic method to synthesize attribution preference data resulting in 95,263 pairs. </small></details>|<sub>ASQA, StrategyQA, ELI5</sub>|
|2024/03/04 <br> (ğŸŒŸğŸŒŸ)  | [Citation-Enhanced Generation for LLM-based Chatbots](https://arxiv.org/pdf/2402.16063) | Weitao Li, Junkai Li, Weizhi Ma, Yang Liu | Tsinghua University | <details><summary><small>This paper presents CEG ...</small></summary><small>This work proposes a post-hoc Citation-Enhanced Generation (CEG) approach combined with RAG. It consists of three components: 1) *Retrieval Augmentation Module* uses NLTK as sentence tokenizer to obtain claims, then uses dense retrieval (SimCSE Bert) to retrieve documents; 2) *Citation Generation Module* first uses NLI model to determine the relationship between each claim-document pair to select valid reference for each claim; 3) *Response Regeneration Module* takes the question, original response, nonfactual claims, and relevant docs, as prompt input to regenerate the new response.</small></details>|<sub>WikiBio GPT-3, FELM, HaluEval, WikiRetr </sub>|

### 1.3 Long-context Enhance <a id="attribution"></a>

| Date       | Title | Authors   | Orgnization | Abs    | Dataset                                                                                           |
|------------|-----------------------------------------------------------------------------------------------------------------|------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|
|2025/06/04 <br> (ğŸŒŸğŸŒŸğŸŒŸğŸŒŸ)  | [Stronger Baselines for Retrieval-Augmented Generation with Long-Context Language Models](https://arxiv.org/pdf/2506.03989) [[code](https://github.com/alex-laitenberger/stronger-baselines-rag/): ![](https://img.shields.io/github/stars/alex-laitenberger/stronger-baselines-rag.svg?style=social)] | Alex Laitenberger, Christopher D. Manning, Nelson F. Liu |  Stanford University | <details><summary><small>This paper compares **DOS RAG** ...</small></summary><small>This paper aims to study ``With long-context LLMs (GPT-4o), do multistage retrieval-augmented generation (RAG) pipelines still offer measurable benefits over simpler, single-stage approaches''. The results show that DOS RAG consistently matches or outperforms more intricate methods on âˆBench, QuALITY, NarrativeQA. </small></details>|<sub>âˆBench, QuALITY, NarrativeQA</sub>|
|2024/09/01 <br> (ğŸŒŸğŸŒŸğŸŒŸ)  | [LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs](https://arxiv.org/pdf/2406.15319) [[code](https://github.com/TIGER-AI-Lab/LongRAG/): ![](https://img.shields.io/github/stars/TIGER-AI-Lab/LongRAG.svg?style=social)] | Ziyan Jiang, Xueguang Ma, Wenhu Chen |  University of Waterloo | <details><summary><small>This paper presents **LongRAG** ...</small></summary><small>The LongRAG consists of a "long retriever" and a "long reader", which processes the entire Wikipedia into 4K-token units, which is 30x longer than before. It adopts off-the-shelf BGE as retriever and Gemini1.5-Pro or GPT-4o as readers without any further tuning. </small></details>|<sub>NQ, HotpotQA, Qasper, MultiFieldQA-en </sub>|
|2024/07/11 <br> (ğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸ)  | [LLM Maybe LongLM: SelfExtend LLM Context Window Without Tuning](https://arxiv.org/pdf/2401.01325) [[code](https://github.com/datamllab/LongLM): ![](https://img.shields.io/github/stars/datamllab/LongLM.svg?style=social)] | ZHongye Jin, Xiaotian Han, Jingfeng Yang, et. al. | Texas A&M University | <details><summary><small>This paper presents SelfExtend ...</small></summary><small>SelfExtend extend the context window of LLMs by construncting bi-level attention information without fine-tuning: 1) The grouped attention captures the dependencies amongo tokens that are far apart; 2) The neighbor attention captures dependencies among adjacent tokens within a specified range</small></details>|<sub>LongBench, L-Eval </sub>|
|2024/05/29 <br> (ğŸŒŸğŸŒŸğŸŒŸ) | [Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models](https://arxiv.org/pdf/2402.02244) | Xindi Wang, Mahsa Salmani, Parsa Omidi, et. al. | Huawei Tech. Canada, University of Western Ontario | <details><summary><small>This paper presents a survey ...</small></summary><small>This paper survey works in enabling LLMs to handle long sequences, including *length extrapolation*, *attention approximation*, *attention-free transformers*, *model compression*, and *hardware-aware transformers*.</small></details>|<sub>None</sub>|

### 1.4 Reasoning Enhance <a id="reason"></a>

| Date       | Title | Authors   | Orgnization | Abs    | Dataset                                                                                           |
|------------|-----------------------------------------------------------------------------------------------------------------|------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|
|2025/05/07 <br> (ğŸŒŸğŸŒŸ) |[ZEROSEARCH: Incentivize the Search Capability of LLMs without Searching](https://arxiv.org/pdf/2505.04588) [[code](https://github.com/Alibaba-NLP/ZeroSearch): ![](https://img.shields.io/github/stars/Alibaba-NLP/ZeroSearch.svg?style=social)]|Hao Sun, Zile Qiao, Jiayan Guo, et al.|Tongyi Lab, Alibaba Group|<details><summary><small>**ZeroSearch**: RL, Qwen2.5.</small></summary><small> This work proposes **ZeroSearch**, a RL framework that incentivizes the search capability of LLMs. It transforms LLM into a retrieval module by supervised fine-tuning.It introduces a curriculum rollout mechanism to progressively elicit model's reasoning ability by exposing it to increasingly challenging retrieval scenarios. The rollout trajectory contains [<think><search><information><answer>].</small></details>|<sub>NQ, TrivaiQA, PopQA, HotpotQA, 2WikiMultiHopQA, Musique, Bamboogle</sub>|
|2025/03/27 <br> (ğŸŒŸğŸŒŸ) |[ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning](https://arxiv.org/pdf/2503.19470) [[code](https://github.com/Agent-RL/ReCall): ![](https://img.shields.io/github/stars/Agent-RL/ReCall.svg?style=social)]|Mingyang Chen, Tianpeng Li, Haoze Sun, et al.| Baichuan Inc.|<details><summary><small>**ReSearch**: RL, Qwen2.5.</small></summary><small> The **ReSearch** trains LLMs to reason with search via RL without any supervised data. The rollout trajectory contains [<think><search><result><answer>].</small></details>|<sub>HotpotQA, 2WikiMultiHopQA, Musique, Bamboogle</sub>|
|2025/03/18 <br> (ğŸŒŸğŸŒŸ) |[R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning](https://arxiv.org/pdf/2503.05592) [[code](https://github.com/RUCAIBox/R1-Searcher): ![](https://img.shields.io/github/stars/RUCAIBox/R1-Searcher.svg?style=social)]|Huatong Song, Jinhao Jiang, Yingqian Min, et al.|Renmin University|<details><summary><small>**R1-Search**: RL, Qwen2.5.</small></summary><small> The **R1-Search** utilizes a two-stage, outcome-based training strategy. The first stage uses retrieve-reward to incentivize the model to conduct retrieval operations. The second stage introduce answer-reward to encourage model to learn to use external retrieval system to solve questions. The rollout trajectory contains [<think><query><document><answer>].</small></details>|<sub>HotpotQA, 2WikiMultiHopQA, Musique, Bamboogle</sub>|
|2025/03/12 <br> (ğŸŒŸğŸŒŸ) |[Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning](https://arxiv.org/abs/2503.09516) [[code](https://github.com/PeterGriffinJin/Search-R1): ![](https://img.shields.io/github/stars/PeterGriffinJin/Search-R1.svg?style=social)]|Bowen Jin, Hansi Zeng, Zhenrui Yue, et al.|UIUC, Google|<details><summary><small>**Search-R1**: RL, Qwen2.5.</small></summary><small> The **Search-R1** optimizes LLM to [search/reason/answer] within multi-turn search interactions, leveraging retrival token masking for stable RL training and a smiple outcome-based reward function.The rollout trajectory contains [<think><search><information><answer>].</small></details>|<sub>NQ, TrivaiQA, PopQA, HotpotQA, 2WikiMultiHopQA, Musique, Bamboogle</sub>|
|2025/01/09 <br> (ğŸŒŸğŸŒŸğŸŒŸ) |[Search-o1: Agentic Search-Enhanced Large Reasoning Models](https://arxiv.org/pdf/2501.05366) [[code](https://github.com/sunnynexus/Search-o1): ![](https://img.shields.io/github/stars/sunnynexus/Search-o1.svg?style=social)]|Xiaoxi Li, Guanting Dong, Jiajie Jin, et al.|Renmin University|<details><summary><small>**Search-o1**: Zeroshot, QwQ-32B.</small></summary><small> The **Search-o1** combines the reasoning process with an agentic RAG mechanism and a knowledge refinement module. The reason-in-document module operates independently from the main reasoning chain, which conducts a thorough analysis of retrieved documents and produces refined information.</small></details>|<sub>GPQA, MATH500, AMC2023, AIME2024, LiveCodeBench, NQ, TriviaQA, HotpotQA, 2WikiMultihopQA, MuSiQue, Bamboogle</sub>|
|2024/09/01 <br> (ğŸŒŸğŸŒŸ)  | [ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates](https://arxiv.org/abs/2502.06772) [[code](https://github.com/Gen-Verse/ReasonFlux/): ![](https://img.shields.io/github/stars/Gen-Verse/ReasonFlux.svg?style=social)] | Ling Yang, Zhaochen Yu, Bin Cui, Mengdi Wang | Princeton University, Peking University | <details><summary><small>**ReasonFlux**: Finetuning, Qwen2.5.</small></summary><small>It train the *ReasonFlux-32B* model with 8 GPUs and introduces three innovations: (i) a structured **thought template library**, containing around 500 high-level thought templates; (ii) performing hierarchical reinforcement learning on a sequence of thought templates, optimizing a base LLM to plan out an optimal template trajectory for gradually handling complex problems; (iii) a brand new inference scaling system that enables hierarchical LLM reasoning by adaptively scaling thought templates at inference time. </small></details>|<sub> MATH, AIME 2024, AMC 2023, OlympiadBench, Gaokao, En 2023 </sub>|

## 2. Haullucinations <a id="hallucinations"></a><br>

| Date       | Title | Authors   | Orgnization | Abs    | Dataset                                                                                           |
|------------|-----------------------------------------------------------------------------------------------------------------|------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|
|2023/09/13 <br> (ğŸŒŸğŸŒŸ)  | [Cognitive Mirage: A Review of Hallucinations in Large Language Models](https://arxiv.org/pdf/2309.06794.pdf) | Hongbin Ye, Tong Liu, Aijia Zhang, Wei Hua, Weiqiang Jia | Zhejiang Lab | <details><summary><small>This paper presents taxonomy of hallucinations ...</small></summary><small>This work provides a literature review on hallucinations, which presents a taxonomy of hallucinations from several text generation tasks, and mechanism analysis (three types: data collection, knowledge gap, and optimization process), detection methods and improvement approaches.</small></details>| <sub>-</sub>|

## 3. Datasets <a id="datasets"></a>

### 3.1 Factoid QA <a id="fqa"></a>

| Date       | Title | Authors   | Orgnization | Abs    | Dataset                                                                                           |
|------------|-----------------------------------------------------------------------------------------------------------------|------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|
|2024/01/26 | [Benchmarking Large Language Models in Complex Question Answering Attribution using Knowledge Graphs](https://arxiv.org/abs/2401.14640.pdf)| Nan Hu, Jiaoyan Chen, Yike Wu, Guilin Qi, Sheng Bi, Tongtong Wu, Jeff Z. Pan.|Southeast University, The University of Manchester,The University of Edinburgh |<details><summary><small>This paper presents CAQA ...</small></summary><small>CAQA is a new benchmark for complex question answering attribution, which is designed to evaluate the ability of LLMs to answer complex questions with the help of knowledge graphs.</small></details>| <sub> CAQA </sub>|
|2022/04/12| [ASQA: Factoid Questions Meet Long-Form Answers](https://arxiv.org/abs/2204.06092.pdf) [[dataset](https://huggingface.co/datasets/din0s/asqa)]|Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, Ming-Wei Chang|Carnegie Mellon University, Duke University, Google Research |<details><summary><small>This paper presents ASQA ...</small></summary><small>ASQA is the first long-form question answering dataset that focuses on ambiguous factoid questions.</small></details>| <sub>ASQA</sub>|

### 3.2 Long-form QA <a id="lfqa"></a>

